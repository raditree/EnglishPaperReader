import requests
import requests.exceptions
import xml.etree.ElementTree as ET
import time
import re
import os
import ssl
import certifi
from datetime import datetime, timezone
from html import unescape
from urllib.parse import urlparse

# ä¿®å¤Windows SSLè¯ä¹¦éªŒè¯é—®é¢˜
ssl_context = ssl.create_default_context(cafile=certifi.where())
ssl._create_default_https_context = lambda: ssl_context


class ArxivRSSFetcher:
    def __init__(self, delay=5, download_pdf=True, pdf_dir="pdfs"):
        self.delay = delay
        self.download_pdf = download_pdf
        self.pdf_dir = pdf_dir
        self.base_url = "https://rss.arxiv.org/rss/"

        # åˆ›å»ºPDFä¸‹è½½ç›®å½•
        if self.download_pdf and not os.path.exists(self.pdf_dir):
            os.makedirs(self.pdf_dir)
            print(f"âœ“ åˆ›å»ºPDFç›®å½•: {self.pdf_dir}")

    def fetch_rss_feed(self, category):
        """è·å–æŒ‡å®šåˆ†ç±»çš„RSS feed"""
        print(f"\næ­£åœ¨è·å– RSS: {category}...")
        url = f"{self.base_url}{category}"

        try:
            max_retries = 3
            base_delay = 1
            success = False
            for attempt in range(max_retries + 1):
                try:
                    response = requests.get(
                        url,
                        timeout=(10, 30),  # 10sè¿æ¥è¶…æ—¶,30sè¯»å–è¶…æ—¶
                        headers={
                            'User-Agent': 'ArxivRSSFetcher/1.0 (research purpose)',
                            'Accept': 'application/rss+xml, application/xml, text/xml'
                        }
                    )
                    response.raise_for_status()
                    success = True
                    break
                except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.HTTPError) as e:
                    if attempt == max_retries:
                        print(f"  Attempt {attempt+1}/{max_retries+1} failed: {str(e)}, æ— æ›´å¤šé‡è¯•")
                        raise
                    delay = base_delay * (2 ** attempt)
                    print(f"  Attempt {attempt+1}/{max_retries+1} failed: {str(e)}, {delay}såé‡è¯•...")
                    time.sleep(delay)
            if not success:
                return []

            papers = self.parse_rss(response.content, category)
            print(f"  âœ“ RSSè§£æå®Œæˆï¼Œè·å– {len(papers)} ç¯‡è®ºæ–‡")
            return papers

        except Exception as e:
            print(f"  âœ— è·å–å¤±è´¥: {e}")
            return []

    def parse_rss(self, xml_content, source_category):
        """è§£æRSS XML"""
        root = ET.fromstring(xml_content)
        ns = {
            'content': 'http://purl.org/rss/1.0/modules/content/',
            'dc': 'http://purl.org/dc/elements/1.1/',
            'atom': 'http://www.w3.org/2005/Atom'
        }

        papers = []
        channel = root.find('channel')

        if channel is None:
            return papers

        feed_title = channel.findtext('title', 'Unknown Feed')
        feed_date = channel.findtext('lastBuildDate', '')

        print(f"  Feedæ ‡é¢˜: {feed_title}")
        print(f"  æ›´æ–°æ—¶é—´: {feed_date}")

        for item in channel.findall('item'):
            paper = {}

            # åŸºæœ¬å­—æ®µ
            paper['title'] = self._clean_text(item.findtext('title', ''))
            paper['link'] = item.findtext('link', '')

            # æå–arXiv ID
            arxiv_id = paper['link'].split('/')[-1] if paper['link'] else ''
            paper['arxiv_id'] = arxiv_id

            # æ„å»ºPDFé“¾æ¥
            paper['pdf_url'] = f"https://arxiv.org/pdf/{arxiv_id}.pdf" if arxiv_id else ''

            # ä½œè€…
            author_text = item.findtext('dc:creator', '', ns)
            if author_text:
                authors = [a.strip() for a in re.split(r',|\band\b', author_text) if a.strip()]
                paper['authors'] = authors
            else:
                paper['authors'] = []

            # æ—¥æœŸ
            paper['pub_date'] = item.findtext('pubDate', '')
            paper['dc_date'] = item.findtext('dc:date', '', ns)

            # åˆ†ç±»
            categories = []
            for cat in item.findall('category'):
                if cat.text:
                    categories.append(cat.text)
            paper['categories'] = categories if categories else [source_category]
            paper['source_category'] = source_category

            # æ‘˜è¦
            description = item.findtext('description', '')
            paper['abstract'] = self._extract_abstract(description)

            papers.append(paper)

        return papers

    def _extract_abstract(self, html_text):
        """ä»HTMLä¸­æå–çº¯æ–‡æœ¬æ‘˜è¦"""
        if not html_text:
            return ""
        text = re.sub(r'<[^>]+>', ' ', html_text)
        text = unescape(text)
        text = re.sub(r'\s+', ' ', text).strip()
        if 'Abstract:' in text:
            text = text.split('Abstract:', 1)[1].strip()
        return text

    def _clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        text = unescape(text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def download_pdf_file(self, paper, category_dir=None):
        """
        ä¸‹è½½PDFæ–‡ä»¶
        è¿”å›: (æˆåŠŸ/å¤±è´¥, æ–‡ä»¶è·¯å¾„/é”™è¯¯ä¿¡æ¯, æ–‡ä»¶å¤§å°)
        """
        if not paper.get('pdf_url'):
            return False, "æ— PDFé“¾æ¥", 0

        arxiv_id = paper['arxiv_id']
        pdf_url = paper['pdf_url']

        # æ„å»ºä¿å­˜è·¯å¾„
        if category_dir:
            save_dir = os.path.join(self.pdf_dir, self._sanitize_filename(category_dir))
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)
        else:
            save_dir = self.pdf_dir

        # æ–‡ä»¶å: arxiv_id.pdf
        filename = f"{arxiv_id}.pdf"
        filepath = os.path.join(save_dir, filename)

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(filepath):
            file_size = os.path.getsize(filepath)
            print(f"    â­ï¸  PDFå·²å­˜åœ¨: {filename} ({self._format_size(file_size)})")
            return True, filepath, file_size

        try:
            print(f"    â¬‡ï¸  æ­£åœ¨ä¸‹è½½ PDF: {arxiv_id}...")

            max_retries =3
            base_delay=1
            for attempt in range(max_retries+1):
                try:
                    response = requests.get(
                        pdf_url,
                        timeout=(10,30),  # 10sè¿æ¥è¶…æ—¶,30sè¯»å–è¶…æ—¶
                        headers={
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                        },
                        stream=True  # æµå¼ä¸‹è½½å¤§æ–‡ä»¶
                    )
                    response.raise_for_status()
                    break
                except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.HTTPError) as e:
                    if attempt == max_retries:
                        print(f"    Attempt {attempt+1}/{max_retries+1} failed: {str(e)}, æ— æ›´å¤šé‡è¯•")
                        raise
                    delay = base_delay*(2**attempt)
                    print(f"    Attempt {attempt+1}/{max_retries+1} failed: {str(e)}, {delay}såé‡è¯•...")
                    time.sleep(delay)

            # è·å–æ–‡ä»¶å¤§å°
            total_size = int(response.headers.get('content-length', 0))

            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'wb') as f:
                downloaded = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)

            # éªŒè¯æ–‡ä»¶
            if os.path.exists(filepath):
                actual_size = os.path.getsize(filepath)
                print(f"    âœ“ ä¸‹è½½å®Œæˆ: {filename} ({self._format_size(actual_size)})")
                return True, filepath, actual_size
            else:
                return False, "æ–‡ä»¶ä¿å­˜å¤±è´¥", 0

        except Exception as e:
            print(f"    âœ— ä¸‹è½½å¤±è´¥: {e}")
            # æ¸…ç†ä¸å®Œæ•´æ–‡ä»¶
            if os.path.exists(filepath):
                os.remove(filepath)
            return False, str(e), 0

    def _sanitize_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        return re.sub(r'[\\/*?:"<>|]', "_", filename)

    def _format_size(self, size_bytes):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        else:
            return f"{size_bytes / (1024 * 1024):.2f} MB"

    def generate_markdown(self, papers_by_category, download_results):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        today_str = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        fetch_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')

        md_content = f"""# arXiv æ¯æ—¥è®ºæ–‡ç²¾é€‰ (RSS + PDF) - {today_str}

> æ•°æ®è·å–æ—¶é—´: {fetch_time}  
> æ•°æ®æº: arXiv RSS Feed  
> åŠŸèƒ½: RSSè·å– + è‡ªåŠ¨PDFä¸‹è½½

## æ¦‚è§ˆ

| åˆ†ç±» | è®ºæ–‡æ ‡é¢˜ | ä½œè€…æ•° | PDFçŠ¶æ€ | æ–‡ä»¶å¤§å° |
|------|----------|--------|---------|----------|
"""

        for cat, papers in papers_by_category.items():
            if papers:
                p = papers[0]
                author_count = len(p['authors'])
                title = p['title'][:35] + '...' if len(p['title']) > 35 else p['title']

                # PDFçŠ¶æ€
                success, path, size = download_results.get(cat, (False, "æœªä¸‹è½½", 0))
                if success:
                    pdf_status = "âœ“ å·²ä¸‹è½½"
                    size_str = self._format_size(size)
                else:
                    pdf_status = "âœ— å¤±è´¥"
                    size_str = "-"
            else:
                title = "æ— æ–°è®ºæ–‡"
                author_count = 0
                pdf_status = "-"
                size_str = "-"

            md_content += f"| `{cat}` | {title} | {author_count} | {pdf_status} | {size_str} |\n"

        md_content += "\n---\n\n"

        # è¯¦ç»†å†…å®¹
        for cat, papers in papers_by_category.items():
            md_content += f"## {cat}\n\n"

            if not papers:
                md_content += "> âš ï¸ è¯¥åˆ†ç±»ä»Šæ—¥æš‚æ— æ–°æäº¤çš„è®ºæ–‡\n\n"
                md_content += "---\n\n"
                continue

            paper = papers[0]

            # æ ‡é¢˜
            md_content += f"### {paper['title']}\n\n"

            # å…ƒä¿¡æ¯
            md_content += "**ä½œè€…:** "
            if len(paper['authors']) <= 3:
                md_content += ", ".join(paper['authors']) if paper['authors'] else "æœªçŸ¥"
            else:
                md_content += ", ".join(paper['authors'][:3]) + f" ç­‰ **{len(paper['authors'])}** ä½"
            md_content += "\n\n"

            md_content += f"**arXiv ID:** [{paper['arxiv_id']}]({paper['link']})\n\n"
            md_content += f"**å‘å¸ƒæ—¥æœŸ:** {paper['pub_date']}\n\n"

            # PDFä¸‹è½½ä¿¡æ¯
            success, path, size = download_results.get(cat, (False, "æœªä¸‹è½½", 0))
            if success:
                md_content += f"**PDFæ–‡ä»¶:** `{path}` ({self._format_size(size)})\n\n"
                md_content += f"**PDFé“¾æ¥:** [{paper['pdf_url']}]({paper['pdf_url']})\n\n"
            else:
                md_content += f"**PDFé“¾æ¥:** [{paper['pdf_url']}]({paper['pdf_url']})\n\n"
                if not success and path != "æœªä¸‹è½½":
                    md_content += f"**ä¸‹è½½çŠ¶æ€:** âŒ {path}\n\n"

            # åˆ†ç±»
            if paper['categories']:
                md_content += "**åˆ†ç±»:** " + ", ".join([f"`{c}`" for c in paper['categories']]) + "\n\n"

            # æ‘˜è¦
            md_content += "#### æ‘˜è¦\n\n"
            if paper['abstract']:
                md_content += f"{paper['abstract']}\n\n"
            else:
                md_content += "> æ‘˜è¦æœªæä¾›\n\n"

            md_content += "---\n\n"

        # æŠ€æœ¯ä¿¡æ¯
        total_papers = sum(1 for p in papers_by_category.values() if p)
        success_downloads = sum(1 for s, _, _ in download_results.values() if s)

        md_content += f"""
## ä¸‹è½½ç»Ÿè®¡

- **æ€»åˆ†ç±»æ•°**: {len(papers_by_category)}
- **æˆåŠŸè·å–RSS**: {total_papers}/{len(papers_by_category)}
- **PDFä¸‹è½½æˆåŠŸ**: {success_downloads}/{total_papers}
- **PDFå­˜å‚¨ç›®å½•**: `{self.pdf_dir}/`

## æŠ€æœ¯ä¿¡æ¯

- **è·å–æ–¹å¼**: Standard RSS 2.0 Feed
- **RSS URL**: `https://rss.arxiv.org/rss/<category>`
- **è¯·æ±‚é—´éš”**: {self.delay} ç§’
- **PDFä¸‹è½½**: è‡ªåŠ¨ä¸‹è½½åˆ° `{self.pdf_dir}/<category>/` ç›®å½•

### æ–‡ä»¶å‘½åè§„åˆ™

PDFæ–‡ä»¶ä»¥arXiv IDå‘½åï¼Œæ ¼å¼ä¸º: `<arxiv_id>.pdf`  
ä¾‹å¦‚: `2501.12345.pdf`

---

*æœ¬æ–‡ä»¶ç”± ArxivRSSFetcher è‡ªåŠ¨ç”Ÿæˆ*  
*æ•°æ®æ¥æºäº arXiv.org*
"""

        return md_content

    # ä¿®å¤ 3: ä¿®æ”¹ save_markdown æ–¹æ³•ä¸­çš„è·¯å¾„ï¼ˆget_passage.py ä¸­ï¼‰

    def save_markdown(self, content, filename=None):
        """ä¿å­˜Markdownæ–‡ä»¶"""
        if filename is None:
            date_str = datetime.now(timezone.utc).strftime('%Y%m%d')
            # ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–ç¡®ä¿ç›®å½•å­˜åœ¨
            log_dir = os.path.join(os.path.dirname(__file__), 'log')
            os.makedirs(log_dir, exist_ok=True)
            filename = os.path.join(log_dir, f"arxiv_daily_rss_{date_str}.md")

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"\nâœ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename

    def run(self, categories):
        """ä¸»è¿è¡Œå‡½æ•°"""
        print("=" * 70)
        print("arXiv RSS + PDF è‡ªåŠ¨ä¸‹è½½å™¨")
        print(f"ç›®æ ‡åˆ†ç±»: {len(categories)} ä¸ª")
        print(f"è¯·æ±‚é—´éš”: {self.delay} ç§’")
        print(f"PDFä¸‹è½½: {'å¯ç”¨' if self.download_pdf else 'ç¦ç”¨'}")
        print(f"PDFç›®å½•: {self.pdf_dir}/")
        print("=" * 70)

        papers_by_category = {}
        download_results = {}

        for i, category in enumerate(categories):
            # 1. è·å–RSS
            papers = self.fetch_rss_feed(category)
            papers_by_category[category] = papers

            # 2. ä¸‹è½½PDFï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰è®ºæ–‡ï¼‰
            if self.download_pdf and papers:
                paper = papers[0]
                print(f"  ğŸ“„ å‡†å¤‡ä¸‹è½½PDF: {paper['arxiv_id']}")
                success, path, size = self.download_pdf_file(paper, category)
                download_results[category] = (success, path, size)
            else:
                download_results[category] = (False, "æ— è®ºæ–‡æˆ–å·²ç¦ç”¨", 0)

            # 3. é—´éš”ç­‰å¾…
            if i < len(categories) - 1 and self.delay > 0:
                print(f"  â³ ç­‰å¾… {self.delay} ç§’...")
                time.sleep(self.delay)

        print("=" * 70)
        print("æ­£åœ¨ç”Ÿæˆ Markdown æŠ¥å‘Š...")

        # ç”Ÿæˆå¹¶ä¿å­˜
        md_content = self.generate_markdown(papers_by_category, download_results)
        filename = self.save_markdown(md_content)

        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 70)
        print("è·å–æ‘˜è¦:")
        print("-" * 70)

        for cat in categories:
            papers = papers_by_category.get(cat, [])
            success, path, size = download_results.get(cat, (False, "æœªä¸‹è½½", 0))

            if papers:
                p = papers[0]
                title = p['title'][:45] + '...' if len(p['title']) > 45 else p['title']
                print(f"  âœ“ {cat:10s} | {title}")
                if success:
                    print(f"    PDF: {self._format_size(size):>10s} | {path}")
                else:
                    print(f"    PDF: ä¸‹è½½å¤±è´¥ - {path}")
            else:
                print(f"  âœ— {cat:10s} | ä»Šæ—¥æ— æ–°è®ºæ–‡")

        print("=" * 70)

        return filename, papers_by_category, download_results


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    # æŒ‡å®šåˆ†ç±»åˆ—è¡¨
    TARGET_CATEGORIES = [
        'cs.AI',  # äººå·¥æ™ºèƒ½
        #'cs.CC',  # è®¡ç®—å¤æ‚æ€§
        #'math.AG',  # ä»£æ•°å‡ ä½•
        #'math.NT',  # æ•°è®º
        #'cs.ET',  # æ–°å…´æŠ€æœ¯
        #'cs.GL',  # ä¸€èˆ¬æ–‡çŒ®
        #'cs.IT',  # ä¿¡æ¯è®º
    ]

    # é…ç½®å‚æ•°
    CONFIG = {
        'delay': 5,  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        'download_pdf': True,  # æ˜¯å¦ä¸‹è½½PDF
        'pdf_dir': 'arxiv_pdfs'  # PDFä¿å­˜ç›®å½•
    }

    # åˆ›å»ºè·å–å™¨å¹¶è¿è¡Œ
    fetcher = ArxivRSSFetcher(**CONFIG)
    filename, results, downloads = fetcher.run(TARGET_CATEGORIES)

    # æœ€ç»ˆæŠ¥å‘Š
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"   MarkdownæŠ¥å‘Š: {filename}")
    print(f"   PDFæ–‡ä»¶ç›®å½•: {CONFIG['pdf_dir']}/")

    # ç»Ÿè®¡
    total_pdfs = sum(1 for s, _, _ in downloads.values() if s)
    print(f"   PDFä¸‹è½½æˆåŠŸ: {total_pdfs}/{len(TARGET_CATEGORIES)}")

    if total_pdfs > 0:
        print(f"\nğŸ’¡ æç¤º:")
        print(f"   PDFæ–‡ä»¶æŒ‰åˆ†ç±»å­˜å‚¨åœ¨ {CONFIG['pdf_dir']}/<åˆ†ç±»å>/ ç›®å½•ä¸‹")
        print(f"   ä¾‹å¦‚: {CONFIG['pdf_dir']}/cs.AI/2501.12345.pdf")